{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Data Bias in Google Jigsaw's Perspective API\n"
      ],
      "metadata": {
        "id": "fuiNSMPzlyiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this assignment is to determine some form of bias in the Perspective API, which is a natural language processing model designed to provide a toxicity score for internet comments."
      ],
      "metadata": {
        "id": "LCU2DuHanNcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from googleapiclient import discovery\n",
        "import json\n",
        "\n",
        "API_KEY = 'AIzaSyC0CKEKkA9fqWsAJ8iUuUQt43lIF233Zxs'\n",
        "\n",
        "client = discovery.build(\n",
        "  \"commentanalyzer\",\n",
        "  \"v1alpha1\",\n",
        "  developerKey=API_KEY,\n",
        "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
        "  static_discovery=False,\n",
        ")\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(\"data_bias_training_data2.csv\")\n",
        "test_df.head()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lFenGXGXuoIg",
        "outputId": "7d49c573-17a0-42ea-e00f-7a39eee6d098"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Type                                Comments Actual Label  Perspective API\n",
              "0   ACC                   wtf that was so crazy    NOT TOXIC              NaN\n",
              "1  FULL         what the fuck that was so crazy    NOT TOXIC              NaN\n",
              "2   ACC                  thats so funny im lmao    NOT TOXIC              NaN\n",
              "3  FULL  thats so funny im laughing my ass off     NOT TOXIC              NaN\n",
              "4   ACC             thats so hilarious im lmfao    NOT TOXIC              NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db35a9f3-6fc5-492f-9036-e77c162c2c4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Actual Label</th>\n",
              "      <th>Perspective API</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ACC</td>\n",
              "      <td>wtf that was so crazy</td>\n",
              "      <td>NOT TOXIC</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FULL</td>\n",
              "      <td>what the fuck that was so crazy</td>\n",
              "      <td>NOT TOXIC</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ACC</td>\n",
              "      <td>thats so funny im lmao</td>\n",
              "      <td>NOT TOXIC</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>FULL</td>\n",
              "      <td>thats so funny im laughing my ass off</td>\n",
              "      <td>NOT TOXIC</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ACC</td>\n",
              "      <td>thats so hilarious im lmfao</td>\n",
              "      <td>NOT TOXIC</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db35a9f3-6fc5-492f-9036-e77c162c2c4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-db35a9f3-6fc5-492f-9036-e77c162c2c4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-db35a9f3-6fc5-492f-9036-e77c162c2c4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-67839aca-0966-4ff6-bd4e-014d7215f89d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67839aca-0966-4ff6-bd4e-014d7215f89d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-67839aca-0966-4ff6-bd4e-014d7215f89d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "toxic_base = 0.5\n",
        "\n",
        "# testing with neutral queries\n",
        "\n",
        "\n",
        "\n",
        "neutral_text_samples =  ['this is a neutral comment' ,\n",
        " 'the sky is blue',\n",
        " 'hello world', 'leaves are green']\n",
        "\n",
        "#iterating through the statemenets and testing the level of toxicity for each\n",
        "\n",
        "for text in neutral_text_samples:\n",
        "  analyze_request = {\n",
        "    'comment': { 'text': text },\n",
        "    'requestedAttributes': {'TOXICITY': {}}\n",
        "  }\n",
        "\n",
        "  response = client.comments().analyze(body=analyze_request).execute()\n",
        "  toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "\n",
        "# printing toxicity scores\n",
        "\n",
        "\n",
        "  if toxicity_score > toxic_base:\n",
        "    print(f'comment: {text}\\n Score: {toxicity_score}\\n TOXIC\\n')\n",
        "  else:\n",
        "    print(f'comment: {text}\\n Score: {toxicity_score}\\n NOT TOXIC \\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv52uRC5epzv",
        "outputId": "37f89c8f-d422-4cf3-b2ae-47810e9a296a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comment: this is a neutral comment\n",
            " Score: 0.020607091\n",
            " NOT TOXIC \n",
            "\n",
            "comment: the sky is blue\n",
            " Score: 0.022610499\n",
            " NOT TOXIC \n",
            "\n",
            "comment: hello world\n",
            " Score: 0.020842785\n",
            " NOT TOXIC \n",
            "\n",
            "comment: leaves are green\n",
            " Score: 0.03309893\n",
            " NOT TOXIC \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, I created a few objective and neutral statements to establish a baseline idea for what the model considers \"toxic\". I also established a threshold of toxicity (above 0.5 being toxic).\n",
        "\n",
        "The code above iterates through a list of statements, extracts them and their corresponding toxicity scores, and saves them as variables  (response and toxicity score). It then displays the score along with the predetermined level of toxcicity that the score corresponds to.\n",
        "\n",
        "The output of the code displays all the statements with very low toxicity scores (all below .1)."
      ],
      "metadata": {
        "id": "qARZHhDdhnjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hypothesis:\n",
        "\n",
        "#### The model will likely score obscenities as much more toxic than other negative language, but, given acronyms that contain obscenities, the model will output a much lower score than the statements that contain the obscenities in full.\n",
        "\n",
        "\n",
        "\n",
        "If the model is unable to correctly identify acronyms as toxic, this could demonstrate age bias. Online, children are the most vulnerable to being exposed to profanities, and they are much more likely to see these in the form of acronyms. Platforms and developers that use Perspective API should then be aware that the API does not account for profane text that is seen in the form of acronyms.\n"
      ],
      "metadata": {
        "id": "_wcSS8_11HOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments = test_df[\"Comments\"]\n",
        "API_labels = []\n",
        "\n",
        "def checking_toxicity(comments):\n",
        "\n",
        "  for text in comments:\n",
        "    analyze_request = {\n",
        "      'comment': { 'text': text },\n",
        "      'requestedAttributes': {'TOXICITY': {}}\n",
        "  }\n",
        "\n",
        "    response = client.comments().analyze(body=analyze_request).execute()\n",
        "    toxicity_score = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "\n",
        "    comment_label = \"TOXIC\" if toxicity_score > toxic_base else \"NOT TOXIC\"\n",
        "    API_labels.append(comment_label)\n",
        "\n",
        "    print(f'comment: {text}\\n Score: {toxicity_score}\\n {comment_label}\\n')\n",
        "\n",
        "\n",
        "checking_toxicity(comments)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdxpJoS_0vr7",
        "outputId": "eb13f23a-3279-4bad-e215-2c04aa610424"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comment: wtf that was so crazy\n",
            " Score: 0.5566829\n",
            " TOXIC\n",
            "\n",
            "comment: what the fuck that was so crazy\n",
            " Score: 0.88599813\n",
            " TOXIC\n",
            "\n",
            "comment: thats so funny im lmao\n",
            " Score: 0.08637348\n",
            " NOT TOXIC\n",
            "\n",
            "comment: thats so funny im laughing my ass off \n",
            " Score: 0.62136006\n",
            " TOXIC\n",
            "\n",
            "comment: thats so hilarious im lmfao\n",
            " Score: 0.112643376\n",
            " NOT TOXIC\n",
            "\n",
            "comment: thats so hilarious im laughing my fucking ass off\n",
            " Score: 0.85333383\n",
            " TOXIC\n",
            "\n",
            "comment: that test was so hard, im going to kms\n",
            " Score: 0.09394901\n",
            " NOT TOXIC\n",
            "\n",
            "comment: that test was so hard, im going to kill myself\n",
            " Score: 0.7308154\n",
            " TOXIC\n",
            "\n",
            "comment: I cant believe we lost, that is such bs\n",
            " Score: 0.24442963\n",
            " NOT TOXIC\n",
            "\n",
            "comment: I cant believe we lost, that is such bullshit\n",
            " Score: 0.8460273\n",
            " TOXIC\n",
            "\n",
            "comment: I have to leave for school rfn\n",
            " Score: 0.08716487\n",
            " NOT TOXIC\n",
            "\n",
            "comment: I have to leave for school right fucking now\n",
            " Score: 0.76197964\n",
            " TOXIC\n",
            "\n",
            "comment: you must be joking, bffr\n",
            " Score: 0.17371799\n",
            " NOT TOXIC\n",
            "\n",
            "comment: you must be joking, be fucking for real\n",
            " Score: 0.8540474\n",
            " TOXIC\n",
            "\n",
            "comment: lfg to the grocery store today \n",
            " Score: 0.038519915\n",
            " NOT TOXIC\n",
            "\n",
            "comment: lets fucking go to the grocery store today \n",
            " Score: 0.687436\n",
            " TOXIC\n",
            "\n",
            "comment: I dont fw my english teacher\n",
            " Score: 0.112540044\n",
            " NOT TOXIC\n",
            "\n",
            "comment: I dont fuck with my english teacher\n",
            " Score: 0.7701451\n",
            " TOXIC\n",
            "\n",
            "comment: that hurt like a mf\n",
            " Score: 0.3561489\n",
            " NOT TOXIC\n",
            "\n",
            "comment: that hurt like a motherfucker\n",
            " Score: 0.9209984\n",
            " TOXIC\n",
            "\n",
            "comment: we need to stfu before the test starts\n",
            " Score: 0.3389984\n",
            " NOT TOXIC\n",
            "\n",
            "comment: we need to shut the fuck up before the test starts\n",
            " Score: 0.8460273\n",
            " TOXIC\n",
            "\n",
            "comment: idfk how to do my homework\n",
            " Score: 0.10913446\n",
            " NOT TOXIC\n",
            "\n",
            "comment: i dont fucking know how to do my homework\n",
            " Score: 0.6407703\n",
            " TOXIC\n",
            "\n",
            "comment: jfk, that outfit looks good!\n",
            " Score: 0.03781283\n",
            " NOT TOXIC\n",
            "\n",
            "comment: jesus fucking christ, that outfit looks good!\n",
            " Score: 0.7252971\n",
            " TOXIC\n",
            "\n",
            "comment: istfg we need to get to work\n",
            " Score: 0.11119666\n",
            " NOT TOXIC\n",
            "\n",
            "comment: i swear to fucking god we need to get to work\n",
            " Score: 0.7252971\n",
            " TOXIC\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code iterates through a list of comments, some of which are actually toxic and some of which are not. Ideally, the model should have the same level of accuracy for comments with acronyms containing profanity that it does for comments with the full text of the profanity.\n",
        "\n",
        "Just looking at the output above, it looks like this is not the case. All of the comments containing profanity are labelled as toxic, while almost none of the comments with acronyms are labelled as toxic.\n",
        "\n",
        "Let's now check the accuracy of the model by comparing the toxic/not toxic labels that the model outputs with the actual toxic/not toxic labels in the test data.\n",
        "\n",
        "To do this, we will examine the percentage of true positive classifiers (comments that were accurately flagged as toxic) and true negative classifiers (comments that were accurately flagged as not toxic) for both the text containing acronyms and the text containing the full text containing profanities."
      ],
      "metadata": {
        "id": "G_2oH6OErVXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Accuracy Checking\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VhNV0M0n0YoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actual_labels = test_df[\"Actual Label\"].tolist()\n",
        "type = test_df[\"Type\"].tolist()\n",
        "\n",
        "ACC_indices = []\n",
        "FULL_indices = []\n",
        "\n",
        "for i in range (len(type)):\n",
        "  if type[i] == \"ACC\":\n",
        "    ACC_indices.append(i)\n",
        "  else:\n",
        "    FULL_indices.append(i)\n",
        "\n",
        "ACC_api_labels = [API_labels[i] for i in ACC_indices]\n",
        "FULL_api_labels = [API_labels [i] for i in FULL_indices]\n",
        "\n",
        "actual_labels = [actual_labels[i] for i in ACC_indices]\n"
      ],
      "metadata": {
        "id": "1SLvn749RUTe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I extracted the indices that contain acronyms and those that contain full text. I then used this to extract the corresponding indices from a previously created list that stored all of the outputted Perspective API labels. I placed these two categories (ACC and FULL) into corresponding lists."
      ],
      "metadata": {
        "id": "jC4g3u1CRbd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(actual_labels, ACC_api_labels, FULL_api_labels):\n",
        "\n",
        "    TP_acronym = 0\n",
        "    TP_full = 0\n",
        "    TN_acronym = 0\n",
        "    TN_full = 0\n",
        "\n",
        "    t_total = 0\n",
        "    nt_total = 0\n",
        "\n",
        "    for i in range(len(actual_labels)):\n",
        "        if actual_labels[i] == \"TOXIC\":\n",
        "            t_total += actual_labels[i] == \"TOXIC\"\n",
        "            TP_acronym += ACC_api_labels[i] == \"TOXIC\"\n",
        "            TP_full += FULL_api_labels[i] == \"TOXIC\"\n",
        "\n",
        "        elif actual_labels[i] == \"NOT TOXIC\":\n",
        "            nt_total += actual_labels[i] == \"NOT TOXIC\"\n",
        "            TN_acronym += ACC_api_labels[i] == 'NOT TOXIC'\n",
        "            TN_full += FULL_api_labels[i] == \"NOT TOXIC\"\n",
        "\n",
        "    try:\n",
        "        percent_TP_acronym = (TP_acronym / t_total) * 100\n",
        "    except ZeroDivisionError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        percent_TP_acronym = 0\n",
        "\n",
        "    try:\n",
        "        percent_TP_full = (TP_full / t_total) * 100\n",
        "    except ZeroDivisionError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        percent_TP_full = 0\n",
        "\n",
        "    try:\n",
        "        percent_TN_acronym = (TN_acronym / nt_total) * 100\n",
        "    except ZeroDivisionError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        percent_TN_acronym = 0\n",
        "\n",
        "    try:\n",
        "        percent_TN_full = (TN_full / nt_total) * 100\n",
        "    except ZeroDivisionError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        percent_TN_full = 0\n",
        "\n",
        "    return percent_TP_acronym, percent_TN_acronym, percent_TP_full, percent_TN_full\n",
        "\n",
        "percent_TP_acronym, percent_TN_acronym, percent_TP_full, percent_TN_full = calculate_accuracy(actual_labels, ACC_api_labels, FULL_api_labels)\n",
        "\n",
        "print(f'Percentage of actually toxic acronyms labelled toxic: {percent_TP_acronym}%')\n",
        "print(f'Percentage of not toxic acronyms labelled not toxic: {percent_TN_acronym}%')\n",
        "print(f'Percentage of actually toxic full text labelled toxic: {percent_TP_full}%')\n",
        "print(f'Percentage of not toxic full text labelled not toxic correctly: {percent_TN_full}%')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiB7A48iTJ2X",
        "outputId": "9d799f52-fec3-431e-8de3-120c5b18edcf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of actually toxic acronyms labelled toxic: 0.0%\n",
            "Percentage of not toxic acronyms labelled not toxic: 83.33333333333334%\n",
            "Percentage of actually toxic full text labelled toxic: 100.0%\n",
            "Percentage of not toxic full text labelled not toxic correctly: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, I counted the number of true positives and true negatives for both the full text comments and the comments containing acronyms. I then compared the number of true negatives and true positives to the total number of negatives and positives and calculated the percent of the time that the model accurately guessed that the comments were positive and negative based on the type of comment (full text or containing an acronym).\n",
        "\n",
        "\n",
        "The model accurately predicted that a statement containing an acronym was toxic 0% of the time. Looking at the previous data, the model only predicted that a comment containing an acronym was toxic once, but, in the test data, this comment was marked not toxic. This proves my hypothesis to be true because the model was unable to identify when a comment that contains profanity hidden in a commonly used acronym was toxic. This means that developers using the perspective API need to be aware that users could potentially be exposed to toxic comments if obscenities are hidden in an acronym. This could represent a form of age bias because, on the internet, children are the most vulnerable to seeing inappropriate language, and could be exposed to such obscenities without their parents being aware.\n",
        "\n",
        "\n",
        "The model accurately predicted that a comment containing an acronym that contains profanity was not toxic 83.3% of the time. In this situation, a false positive, as seen with the lack of true negatives, is much more harmful than a false negative. This means that, although the model did accurately classify many comments that contain acronyms with obscenities as not toxic, the vulnerability in the model is still obvious, and the model could still cause unintentional harm.\n",
        "\n",
        "The model accurately predicted that a comment containing full text profanities was toxic 100% of the time. In fact, every comment containing a full text profanity was labelled toxic. Although this definitely proves the models ability to flag profane language to a degree, it still does not make up for the potential deficiencies in the model that could cause harm due to its inability to flag comments with acronyms; the statements \"that test was so hard i'm going to kms\" and \"that test was so hard, im going to kill myself\", can cause just as much harm because they have the same meaning, but the former received a toxicity score of 0.24 and the latter received a score of 0.73.\n",
        "\n",
        "\n",
        "Finally, the model accurately predicted that a comment containing full text profanities were not toxic 0% of the time. Because the model does not understand social cues or slang, it is understanable, that many of the comments with profanities are labelled as toxic. As I said before, a false negative, in this situation is much more harmful than a false positive. Because of this, deficiencies in the models ability to accurately flag comments containing profanities as not toxic are much less concerning, and can be seen as a potential strength of the model.\n",
        "\n",
        "Generally, the model is likely to flag comments as toxic if they contain any harmful words, regardless of the content of the rest of the statement. This means that the model is likely unable to identify slang words like \"cringe\" or \"cheugy\" as toxic as well. However, this means that neutral statements containing words with double meanings (for instance, \"the bitch just gave birth to a litter of puppies\") would also likely be inaccurately flagged as toxic. Additionally, this means that users on platforms that use Perspective API could get around the models flags by hiding their language in acronyms, as seen throughout the dataset.\n"
      ],
      "metadata": {
        "id": "rut6TG_USDon"
      }
    }
  ]
}